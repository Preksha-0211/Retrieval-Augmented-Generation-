{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3ad18e-a3f5-4508-8607-2572a1c42eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import ollama\n",
    "import chromadb\n",
    "from langchain import hub\n",
    "from operator import itemgetter\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f1a1ba3-a979-4a96-b2bf-7b657afb0e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents for RAG system\n",
    "documents = [\n",
    "    Document(page_content=\"RAG combines information retrieval with LLMs to generate more accurate and context-aware responses. In RAG, relevant documents are retrieved and used as input to improve the LLM's performance.RAG enhances the accuracy by providing real-time access to up-to-date information.\"),\n",
    "    Document(page_content=\"LLMs are AI models trained on vast datasets to understand and generate human-like text. LLMs excel at tasks like language translation, summarization, and question answering. With vast knowledge, LLMs can assist in automating content creation and customer support.\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22ba2e9e-0838-4279-9f4c-e01508b12d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sathw\\anaconda3\\lib\\site-packages\\langsmith\\client.py:333: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "C:\\Users\\sathw\\anaconda3\\lib\\site-packages\\langsmith\\client.py:5431: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  prompt = loads(json.dumps(prompt_object.manifest))\n"
     ]
    }
   ],
   "source": [
    "# Sentence-based text splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=70, chunk_overlap=20)   # chunk_overlap = Overlaps the chunk to help us chain chunck together\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Generate embeddings and create vectorstore\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.1\")\n",
    "vectorstore = Chroma.from_documents(splits, embeddings)\n",
    "\n",
    "# Create a retriever from the vectorstore\n",
    "retriever = vectorstore.as_retriever()        #Retriver is used to find chunks in the vector store that are similar to the given question.\n",
    "\n",
    "# The 'prompt' variable is a pre-configured prompt that can be used to generate answers to questions by using retrieved contextual information.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "# Chain      \n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser() #Parser is used to extract the text answer by using output parser.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ebdfb6c-0c1e-42e3-98f8-12ea5bacc4de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs (Large Language Models) can assist in automating content creation, improving performance through input from retrieved documents, and generating accurate responses for tasks such as question answering and customer support. They can also be used to improve the performance of Retrieval-Augmented Generation (RAG) models. Overall, LLMs are designed to facilitate various tasks related to language understanding and generation.\n"
     ]
    }
   ],
   "source": [
    "# Define a question and invoke the chain\n",
    "question = \"What are the task of LLM?\"\n",
    "result = chain.invoke({'question': question})\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe68b09-f881-4ff4-9061-1a516225b9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The provided context does not mention llamas or their family relationships. It appears to be about Large Language Models (LLMs) and information retrieval.\n"
     ]
    }
   ],
   "source": [
    "# Define a question and invoke the chain\n",
    "question = \"What family is a llama related to?\"\n",
    "result = chain.invoke({'question': question})\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a0a30-b46e-400f-99c8-0ab674ac13f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
